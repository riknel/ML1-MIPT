{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red; font-size: 14pt\">Дедлайн: 20 марта 23:59</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Машинное обучение, ФИВТ, Весна 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall <номер_группы> <фамилия>``, к примеру -- ``ML2018_fall 596 ivanov``\n",
    "- Выполненное дз сохраните в файл ``ML2018_<фамилия>_<группа>_task<номер задания>.ipnb``, к примеру -- ``ML2018_ivanov_596_task1.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com`` (или в телеграм-канал)\n",
    "- Укажите тему письма в следующем формате ``ML2018_fall Question <Содержание вопроса>``\n",
    "\n",
    "--------\n",
    "- **PS1**: Используются автоматические фильтры, мы не найдем ваше дз, если вы укажете тему письма в неправильном формате.\n",
    "- **PS2**: Просроченный дедлайн снижает максимальный вес задания по формуле, указнной на первом семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Теоретические задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30% баллов за задание, оценочное время выполнения 30 минут"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, что мы решаем задачу бинарной классификации и что у нас есть три алгоритма $b_1(x)$, $b_2(x)$ и $b_3(x)$, каждый из которых ошибается с вероятностью p. Мы строим композицию взвешенным голосованием: алгоритмам присвоены значимости $w_1$, $w_2$ и $w_3$, и для вынесения вердикта суммируются значимости алгоритмов, проголосовавших за каждый из классов:\n",
    "\n",
    "$$a_0 = \\sum_{i=1}^3 w_i [b_i(x)=0]$$\n",
    "$$a_1 = \\sum_{i=1}^3 w_i [b_i(x)=1]$$\n",
    "\n",
    "\n",
    "Объект $x$ относится к классу, для которого такая сумма оказалась максимальной. Например, если первые два алгоритма голосуют за класс $0$, а третий — за класс $1$, то выбирается класс $0$, если $w_1 + w_2 > w_3$, и класс $1$ в противном случае. Какова вероятность ошибки такой композиции этих трех алгоритмов, если:\n",
    "1. $w_1 = 0.2, w_2 = 0.3, w_3 = 0.2$;\n",
    "2. $w_1 = 0.2, w_2 = 0.5, w_3 = 0.2$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) <br>\n",
    "Обозначим итоговый алгоритм a(x)\n",
    "Заметим, что в данном случае весов для того, что выбрать j-ый класс нужно, чтобы хотя бы 2 из 3ех базовых алгоритмов проголосовали за него. \n",
    "$$P(a(x) \\neq y) = P(хотя \\; бы \\;2\\; из \\;3ех\\; базовых\\; алгоритмов\\; проголосовали\\; за \\;1-y \\;на\\; входе \\;x) = P(хотя \\; бы \\;2\\; из \\;3ех\\; базовых\\; алгоритмов\\; ошиблись \\;на\\; x) = C_3^2 p^2 = 3p^2$$ <br>\n",
    "2) <br>\n",
    "Заметим, что в данном случае весов для того, что выбрать j-ый класс нужно, чтобы алгоритм $b_2$ проголосовал за него, ответ остальных неважен. Поэтому:\n",
    "$$P(a(x) \\neq y) = P(b_2(x) \\neq y) = p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу бинарной классификации. Будем считать, что все алгоритмы из базового семейства возвращают ответы из отрезка $[0,1]$, которые можно интерпретировать как вероятности принадлежности объектов классу $1$. В качестве функции потерь возьмем отрицательный логарифм правдоподобия:\n",
    "$$L(y,z) = -(y \\log{z}+(1-y)\\log{(1-z)})$$\n",
    "В формуле $y$ - правильный ответ, $z$ - ответ алгоритма. Выпишите формулы для поиска базовых алгоритмов $b_n$ и коэффициентов $\\gamma_n$ в градиентном бустинге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы уже построили $a_{N-1}(x) = \\sum_{i=0}^{N-1} b_i(x)$. <br>\n",
    "Тогда наша задача $$\\sum_{i=1}^l = L(y_i, a_{N-1}(x_i) + b(x_i)) \\rightarrow min_{b}$$\n",
    "$b$ можно представить вектором сдвигов: $ s = (s_1, ..., s_l) = (b(x_1), ...., b(x_l))$ \n",
    "То есть: $$F(s) = \\sum_{i=1}^l = L(y_i, a_{N-1}(x_i) + s_i) \\rightarrow min_{s_1, ..., s_l}$$<br>\n",
    "Нужно найти вектор s, который больше всего уменьшает функцию L - это антиградиент: <br>\n",
    "$s = -grad\\; F(s) = (-L'_z(y_1, a_{N-1}(x_1)), ..., -L'_z(y_l, a_{N-1}(x_l)))$ <br>\n",
    "В нашем случае: $L'_z(y, z) = -\\frac{y}{z} + \\frac{1-y}{1-z} \\; \\Rightarrow \\; s = (-\\frac{y_1}{a_{N-1}(x_1)} + \\frac{1-y_1}{1-a_{N-1}(x_1)}, ..., -\\frac{y_l}{a_{N-1}(x_l)} + \\frac{1-y_l}{1-a_{N-1}(x_l)})$ <br>\n",
    "То есть возьмем \n",
    "$$b_N = (b_N(x_1), ...., b_N(x_l)) = (-\\frac{y_1}{a_{N-1}(x_1)} + \\frac{1-y_1}{1-a_{N-1}(x_1)}, ..., -\\frac{y_l}{a_{N-1}(x_l)} + \\frac{1-y_l}{1-a_{N-1}(x_l)})$$\n",
    "где $a_{N-1}(x) = \\sum_{i=0}^{N-1} b_i(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3 (10% баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известно, что на $n$-й итерации двухклассового метода AdaBoost\n",
    "был выбран базовый классификатор, допускающий ошибку только на одном объекте $x_j$. Найдите нормированный вес $w_j^{(n+1)}$ при этом объекте на следующей итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть всего объектов N. Для простоты обозначим $w_j^n = w_j$. Тогда $err_n = \\frac{w_j}{\\sum_{i=0}^N w_i}$ <br>\n",
    "Тогда \n",
    "$$\\alpha_n = log(\\frac{1 - err_n}{err_n}) = log(1 - err_n) - log(err_n) = log(\\sum_{i=0}^N w_i - w_j) - log(\\sum_{i=0}^N w_i) - log(w_j) + log(\\sum_{i=0}^N w_i) = log(\\sum_{i=0}^N w_i - w_j) - log(w_j)$$ <br>\n",
    "Тогда получим: \n",
    "$$w_j^{n+1} = w_j e^{\\alpha_n * 1} =  w_j e^{log(\\sum_{i=0}^N w_i - w_j) - log(w_j)} = w_j \\frac{\\sum_{i=0}^N w_i - w_j}{w_j} = \\sum_{i=0}^N w_i - w_j = \\sum_{i=0, i \\neq j}^N w_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 70% баллов за задание, оценочное время выполнения 3 часа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Необходимо реализовать класс `RandomForest`** (для решения задачи классификации)\n",
    "\n",
    "**Спецификация:**\n",
    "- класс наследуется от `sklearn.BaseEstimator`;\n",
    "- конструктор содержит следующие параметры: \n",
    "    - `num_trees` - количество деревьев в лесе;\n",
    "    - `max_depth` - максимальная глубина дерева (по умолчанию - `numpy.inf`); \n",
    "    - `max_features` - количество признаков, принимаемое к рассмотрению при разбиении (аналогичный параметр есть в sklearn имплементации). Параметр может принимать значения:\n",
    "        - int - тогда рассматриваем max_features признаков при каждом разбиении;\n",
    "        - float - max_features обозначает процент, int(max_features * n_features) признаков рассматривается при каждом разбиении;\n",
    "        - “sqrt” - max_features=sqrt(n_features);\n",
    "        - “log2” - max_features=log2(n_features);\n",
    "        - None - max_features=n_features;\n",
    "    - `criterion` - критерий разбиения (для классификации - 'gini' или 'entropy', по умолчанию - 'gini'); функции с подсчетом энтропийного и критерия Джини можно взять из предыдущего дз;\n",
    "    \n",
    "- класс имеет методы `fit` и `predict`;\n",
    "- метод `fit` принимает матрицу объектов `X` и вектор ответов `y` (объекты `numpy.ndarray`) и возвращает экземпляр класса\n",
    "    `RandomForest`, представляющий собой Random Forest, обученный по выборке `(X, y)` с учётом заданных в конструкторе параметров; \n",
    "- метод `predict` принимает матрицу объектов и возвращает вектор предсказанных ответов;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagging(X, y, size):\n",
    "    #Implement random sampling here\n",
    "    indeces = np.random.choice(np.arange(y.shape[0]), size=size)\n",
    "    sample_X, sample_y = X[indeces], y[indeces]\n",
    "    return sample_X, sample_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, num_trees, max_depth, max_features, criterion):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        Create trees here, using bagging and RSM.\n",
    "        '''\n",
    "        for num in range(self.num_trees):\n",
    "            sample_X, sample_y = bagging(X_train, y_train, len(X_train))\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.max_features, criterion=self.criterion)\n",
    "            tree.fit(sample_X, sample_y)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Predict the label here using your grown trees.\n",
    "        '''\n",
    "        all_predictions = []\n",
    "        y_pred = np.zeros(len(X_test))\n",
    "        for tree in self.trees:\n",
    "            all_predictions.append(tree.predict(X_test))\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        for i, y in enumerate(all_predictions.T):\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            y_pred[i] = unique[np.argmax(counts)]\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите датасет Wine Data Set (https://archive.ics.uci.edu/ml/datasets/wine). Разделите выборку на обучающую и тестовую с помощью метода `train_test_split`, используйте значения параметров `test_size=0.2`, `random_state=42`. Попробуйте обучить Random Forest на предложенном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0      1    14.23        1.71  2.43               15.6        127   \n",
       "1      1    13.20        1.78  2.14               11.2        100   \n",
       "2      1    13.16        2.36  2.67               18.6        101   \n",
       "3      1    14.37        1.95  2.50               16.8        113   \n",
       "4      1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', \n",
    "         'Nonflavanoid phenols','Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines','Proline'] \n",
    "data = pd.read_csv('wine.data', names=names)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = np.array(data.iloc[:,1:])\n",
    "y = np.array(data.label)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = RandomForest(num_trees = 10, max_depth=10000, max_features='sqrt', criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814814814814815"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажите, как менялись значения критерия качества `accuracy` при увеличении параметра num_trees. Видны ли следы переобучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCkAAAHkCAYAAAATuHWqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "nums = np.arange(10, 1000, 100)\n",
    "for num in nums:\n",
    "    clf = RandomForest(num_trees = num, max_depth=10000, max_features='sqrt', criterion='gini')\n",
    "    clf.fit(X_train, y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    scores.append(accuracy_score(predictions, y_test))\n",
    "    \n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(nums, scores)\n",
    "plt.xlabel('num_trees')\n",
    "plt.ylabel('scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из графика, никакого переобучения нет. Также можно заметить, что уже при около 150 деревьев алгоритм дает 100% результат на наших данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните качество работы вашей реализации RandomForest и реализации из sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем 3 попытки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest из sklearn : 0.9259259259259259\n",
      "Мой RandomForest 0.9629629629629629\n",
      "\n",
      "RandomForest из sklearn : 0.9814814814814815\n",
      "Мой RandomForest 0.9814814814814815\n",
      "\n",
      "RandomForest из sklearn : 1.0\n",
      "Мой RandomForest 0.9814814814814815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "for i in range(3):\n",
    "    clf1 = RandomForestClassifier(n_estimators= 10, max_depth=10000, max_features='sqrt', criterion='gini')\n",
    "    clf1.fit(X_train, y_train)\n",
    "    pred = clf1.predict(X_test)\n",
    "\n",
    "    clf2 = RandomForest(num_trees = 10, max_depth=10000, max_features='sqrt', criterion='gini')\n",
    "    clf2.fit(X_train, y_train)\n",
    "    pred2 = clf2.predict(X_test)\n",
    "\n",
    "    print('RandomForest из sklearn :', accuracy_score(pred, y_test))\n",
    "    print('Мой RandomForest', accuracy_score(pred2, y_test))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, наш RandomForest работает в среднем ничуть не хуже стандартного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модификация Random Forest (15%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измените свою реализацию `RandomForest` так, чтобы случайное подмножество признаков выбиралось не в каждом сплите, а перед построением всего дерева. Сравните результат работы с обычным RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestNew(sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin):\n",
    "    def __init__(self, num_trees, max_depth, max_features, criterion):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        Create trees here, using bagging and RSM.\n",
    "        '''\n",
    "        if (self.max_features == 'sqrt'):\n",
    "            size = int(np.sqrt(len(X_train)))\n",
    "        elif (self.max_features == 'log2'):\n",
    "            size = int(np.log2(len(X_train)))\n",
    "        elif (type(self.max_features) == float):\n",
    "            size = int(self.max_features * len(X_train))\n",
    "        elif (type(self.max_features) == int):\n",
    "            size = self.max_features\n",
    "        self.ind_features = np.random.choice(np.arange(X_train.shape[1]), size=size)\n",
    "        X_train_new = X_train[:,self.ind_features]\n",
    "        for num in range(self.num_trees):\n",
    "            sample_X, sample_y = bagging(X_train_new, y_train, len(X_train))\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=X_train_new.shape[1], \n",
    "                                          criterion=self.criterion)\n",
    "            tree.fit(sample_X, sample_y)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Predict the label here using your grown trees.\n",
    "        '''\n",
    "        all_predictions = []\n",
    "        y_pred = np.zeros(len(X_test))\n",
    "        for tree in self.trees:\n",
    "            all_predictions.append(tree.predict(X_test[:,self.ind_features]))\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        \n",
    "        for i, y in enumerate(all_predictions.T):\n",
    "            unique, counts = np.unique(y, return_counts=True)\n",
    "            y_pred[i] = unique[np.argmax(counts)]\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestNew(num_trees = 10, max_depth=10000, max_features='sqrt', criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что результат стал гораздо хуже. Это логично, так как часть данных теперь мы просто не используем."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
